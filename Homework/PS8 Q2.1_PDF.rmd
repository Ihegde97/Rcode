---
title: "HA8"
author: "Ishwara Hegde, Jonathan Nieman, Aleksei Samkov"
date: "30/11/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning=FALSE)
#As long as this rmd file is in the same folder as the cps_data it should
#run fine

#setwd("~/CEMFI 2020-2021/Term 4 (Sep-Dec)/Microeconometrics/Problem Sets/PS8/")

#load all packages necessary in one go
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse, data.table, modelsummary,kableExtra,synthdid,
               patchwork,mvtnorm)

rm(list = ls())
set.seed(925510415)
```

### 1. Load and normalize data

```{r load_norm}
# Load data
load("cps_data.Rdata")

# Define initial parameters
n <- dim(Y)[1]
T <- dim(Y)[2]
T_0 <- 30

# Normalize Y
Y_norm <- (Y-mean(Y))/sd(Y)
```

### 2. Construct SVD
```{r svd}
Y_svd <- svd(Y_norm)

M <- Y_svd$u[,1:4] %*% diag(Y_svd$d[1:4]) %*% t(Y_svd$v[,1:4])
E <- Y_svd$u[,5:T] %*% diag(Y_svd$d[5:T]) %*% t(Y_svd$v[,5:T])

sig2_e <- sum(E^2)/(n*T)

# Confirm M+E equals normalized Y
max(abs(Y_norm-(M+E)))
```
### 3. Decompose M into two components

```{r decompose}
n_ones <- matrix(1, nrow=n, ncol=n)
T_ones <- matrix(1, nrow=T, ncol=T)

F <- (n_ones %*% M/n) + (M %*% T_ones/T)
L <- M-F
```

### 4. Run a logit of $D_i$ on ${u_1-u_4}$ and extract predicted probabilities
```{r logit}
u <- Y_svd$u[,1:4]
pi <- glm(D~u, family="binomial")$fitted.values
```

### 5. Define DGP function

```{r dgp1}
DGP_1 <- function(n, T, a_0, a_1, F, L, sig2_e, pi) {
  A <- rbinom(n, 1, pi)
  epsilon <- matrix(rnorm(T*n, mean=0, sd=sqrt(sig2_e)), n, T)

  Y <- a_0*F + a_1*L + epsilon 
  W <- cbind(matrix(0, nrow=n, ncol=T_0), matrix(1, nrow=n, ncol=T-T_0)) * A

  data_returned <- list("Y" = Y, "W" = W)
  return(data_returned)
}
```

### 6. Simulate data and compute estimators

```{r sim1}
B <- 400
tau_hat <- matrix(0, nrow = B, ncol = 3)
colnames(tau_hat) <- c("tau_DID", "tau_SC", "tau_SDID")

for (b in 1:B){
  draw <- DGP_1(n, T, 1, 1, F, L, sig2_e, pi)
  rowsort <- order(draw$W[,T])

  data_Y <- draw$Y[rowsort,]
  data_W <- draw$W[rowsort,]
  
  n_0 <- sum(draw$W[,T]==0)
  
  tau_hat[b,1] <-      did_estimate(data_Y, N0 = n_0, T0 = T_0)
  tau_hat[b,2] <-       sc_estimate(data_Y, N0 = n_0, T0 = T_0)
  tau_hat[b,3] <- synthdid_estimate(data_Y, N0 = n_0, T0 = T_0)
}

```

### 7. Plot the distribution of DID and SDID estimators.

```{r plots1}
# Define plotting function
plotting_func <- function(data, x_string){
  p <- ggplot(data=data, mapping=aes(x=x_string)) +
    geom_density(kernel="gaussian", adjust=1.8) + 
    geom_vline(xintercept=0, linetype="dotted", color="blue") +
    theme_minimal()
  return(p)
}

# Create the plots for DGP 1 
tau_hat <- data.table(tau_hat)
p1 <- plotting_func(data = tau_hat, x_string = tau_hat$tau_DID) + xlab("DiD") + xlim(-.15,.15)
p2 <- plotting_func(data = tau_hat, x_string = tau_hat$tau_SDID) + xlab("Synthetic DiD") + xlim(-.15,.15)

p_dgp1 <- p1 + p2 + plot_annotation(
  title = "DGP 1: Distribution of Estimator Bias",
  subtitle = "alpha_0 = alpha_1 = 1"
)

p_dgp1
```

It is clear that the bias of the diff-in-diff estimator is larger both in terms
of its mean and its variance. Allowing the weights assigned to each data point
to vary in both dimensions helps to correct for the fact that our treated 
subsample does not follow perfect parallel trends with the untreated subsample.
It also helps to correct for imbalances in the propensity for treatment across 
the treated anduntreated subsamples.

### 8. Repeat DGP and estimation for alternative alpha parametrizations.

```{r sim2}
tau_hat2 <- matrix(0, nrow = B, ncol = 3)
tau_hat3 <- matrix(0, nrow = B, ncol = 3)
colnames(tau_hat2) <- c("tau_DID", "tau_SC", "tau_SDID")
colnames(tau_hat3) <- c("tau_DID", "tau_SC", "tau_SDID")

for (b in 1:B){
  # Alternative Draw #1 - alpha_0 = 1, alpha_1 = 0
  draw <- DGP_1(n, T, 1, 0, F, L, sig2_e, pi)
  
  rowsort <- order(draw$W[,T])
  data_Y <- draw$Y[rowsort,]
  data_W <- draw$W[rowsort,]
  
  n_0 <- sum(draw$W[,T]==0)
  
  tau_hat2[b,1] <-      did_estimate(data_Y, N0 = n_0, T0 = T_0)
  tau_hat2[b,2] <-       sc_estimate(data_Y, N0 = n_0, T0 = T_0)
  tau_hat2[b,3] <- synthdid_estimate(data_Y, N0 = n_0, T0 = T_0)

  # Alternative Draw #2 - alpha_0 = 0, alpha_1 = 1
  draw <- DGP_1(n, T, 0, 1, F, L, sig2_e, pi)
  
  rowsort <- order(draw$W[,T])
  data_Y <- draw$Y[rowsort,]
  data_W <- draw$W[rowsort,]
  
  n_0 <- sum(draw$W[,T]==0)
  
  tau_hat3[b,1] <-      did_estimate(data_Y, N0 = n_0, T0 = T_0)
  tau_hat3[b,2] <-       sc_estimate(data_Y, N0 = n_0, T0 = T_0)
  tau_hat3[b,3] <- synthdid_estimate(data_Y, N0 = n_0, T0 = T_0)
}

summary(tau_hat)
summary(tau_hat2)
summary(tau_hat3)
```

Each parametrization helps to reveal a bit more about the bias issues in each 
estimation strategy. First, we should understand the interpretation of
$\alpha_0$ and $\alpha_1$.  $\alpha_0$ determines whether or not the data draws
include $\textbf{F}$, and $\alpha_1$ does the same for 
$\textbf{L}=\textbf{M}-\textbf{F}$. Note that $\textbf{M}$ is the sum of the 
four largest components of the SVD of $\tilde{\textbf{Y}}$.  

When $\alpha_0=\alpha_1=1$, the synthetic diff-in-diff estimator is the least
biased and most efficient. This is because our data 

When $\alpha_0=1$ and $\alpha_1=0$, the regulator diff-in-diff estimator 
is the best estimator but is only marginally better than synthetic diff-in-diff.

When $\alpha_0=0$ and $\alpha_1=1$, the synthetic controls estimator is the best
but again is only very slightly better than synthetic diff-in-diff.

### 9. Repeat parts 5-7 but with an alternative DGP
```{r, dgp2}
# Define new DGP function (using a constant average probability instead of individual predictions)
DGP_2 <- function(n, T, a_0, a_1, F, L, sig2_e, pi) {
  
  pibar <- mean(pi)
  
  A <- rbinom(n, 1, pibar)
  epsilon <- matrix(rnorm(T*n, mean=0, sd = sqrt(sig2_e)), n, T)
  
  Y <- a_0*F + a_1*L + epsilon 
  W <- cbind(matrix(0, nrow=n, ncol=T_0), matrix(1, nrow=n, ncol=T-T_0)) * A
  
  data_returned <- list("Y" = Y, "W" = W)
  return(data_returned)
}
``` 

```{r sim3}
# Run the DGP simulation and estimation process again
tau_hat4 <- matrix(0, nrow = B, ncol = 3)
colnames(tau_hat4) <- c("tau_DID", "tau_SC", "tau_SDID")

for (b in 1:B){
  draw <- DGP_2(n, T, 1, 1, F, L, sig2_e, pi)
  rowsort <- order(draw$W[,T])
  
  data_Y <- draw$Y[rowsort,]
  data_W <- draw$W[rowsort,]
  
  n_0 <- sum(draw$W[,T]==0)
  
  tau_hat4[b,1] <-      did_estimate(data_Y, N0 = n_0, T0 = T_0)
  tau_hat4[b,2] <-       sc_estimate(data_Y, N0 = n_0, T0 = T_0)
  tau_hat4[b,3] <- synthdid_estimate(data_Y, N0 = n_0, T0 = T_0)
}
``` 

```{r plots2,echo =FALSE}
# Create the plots for DGP 1 
tau_hat4 <- data.table(tau_hat4)
p4 <- plotting_func(data=tau_hat4,x_string = tau_hat4$tau_DID) + 
        xlab("DiD") + xlim(-.15,.15)
p5 <- plotting_func(data=tau_hat4,x_string = tau_hat4$tau_SDID) +
        xlab("Synthetic DiD") + xlim(-.15,.15)

p_dgp2 <- p4 + p5 + plot_annotation(
  title = "DGP 2: Distribution of Estimator Bias",
  subtitle= "alpha_0 = alpha_1 = 1"
)

p_dgp2
```

Under the new DGP where the propensity for treatment is identical across all 
observations, the bias issue for the diff-in-diff estimator largely vanishes. 
However, the synthetic diff-in-diff estimator remains more efficient, just as it
was for the original DGP with heterogeneity in treatment propensity.
